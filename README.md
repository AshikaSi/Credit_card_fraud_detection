# üí≥ Credit Card Fraud Detection using Machine Learning & PyCaret  

---

## üìò Overview  

This project focuses on detecting **fraudulent credit card transactions** using both **traditional machine learning techniques** and **automated ML with PyCaret**.  

The dataset used is the [**Kaggle Credit Card Fraud Detection Dataset**](https://www.kaggle.com/mlg-ulb/creditcardfraud), which is **highly imbalanced**, containing a very small percentage of fraud cases.  

To handle this challenge, the project is implemented in **two complementary approaches**:  
1Ô∏è‚É£ **Manual ML Pipeline** ‚Äì Handling data imbalance using techniques like **SMOTE**, **ADASYN**, and **Random Over/Undersampling**, followed by model training and evaluation.  
2Ô∏è‚É£ **PyCaret Approach** ‚Äì Leveraging **PyCaret‚Äôs Classification and Anomaly Detection modules** to automatically compare multiple models (**supervised and unsupervised**) for fraud detection.  

---

## ‚öôÔ∏è Tech Stack  

**Language:** Python  
**Platform:** Jupyter Notebook  
**Dataset:** Kaggle ‚Äì Credit Card Fraud Detection  

**Libraries Used:**  
- üß© *Machine Learning:* `scikit-learn`, `imbalanced-learn`, `xgboost`, `lightgbm`, `catboost`  
- ‚ö° *AutoML:* `pycaret`  
- üìä *Visualization:* `matplotlib`, `seaborn`, `plotly`  
- üßÆ *Data Handling:* `pandas`, `numpy`  

---

## üß† Project Approaches  

### 1Ô∏è‚É£ Manual Machine Learning Workflow  

#### **Steps:**  
- Performed **data cleaning**, **EDA**, and visualized the **class imbalance**.  
- Applied **feature scaling** and **train-test split**.  
- Used different **resampling strategies** to handle imbalance:  
  - SMOTE (Synthetic Minority Oversampling)  
  - ADASYN (Adaptive Synthetic Sampling)  
  - Random Oversampling / Undersampling  
- Trained multiple **classification models**:  
  - Logistic Regression  
  - Decision Tree  
  - Random Forest  
  - XGBoost  
  - LightGBM  
  - CatBoost  
  - SVM  

#### **Evaluation Metrics:**  
- Accuracy  
- Precision  
- Recall  
- F1-Score  
- ROC-AUC Curve  

üìà *Special focus was given to maximizing **Recall**, since missing a fraud is costlier than a false alarm.*  

---

### 2Ô∏è‚É£ Automated Approach using PyCaret  

PyCaret simplifies the entire ML workflow ‚Äî from setup to model comparison ‚Äî in just a few lines of code.  

#### **(a) Supervised Model Comparison**  

```python
from pycaret.classification import *
clf = setup(data=df, target='Class', session_id=123)
best_model = compare_models()
```

üìä PyCaret Automated Modeling  

PyCaret automatically trains and ranks multiple classification models  
(Logistic Regression, Random Forest, LightGBM, XGBoost, etc.)  
based on metrics like **AUC**, **F1**, and **Precision-Recall**.  

---

#### **(b) Unsupervised / Anomaly Detection**  

To explore fraud detection **without labels**, PyCaret‚Äôs **Anomaly Detection** module was used:  

```python
from pycaret.anomaly import *
anom = setup(data=df, session_id=123)
iforest = create_model('iforest')     # Isolation Forest
results = assign_model(iforest)
plot_model(iforest, plot='tsne')
```
#### **Models Used**
 - Isolation Forest
 - DBSCAN
 - KMeans
 - One-Class SVM
 - PCA-based Detection

üìä Anomalies identified by these models were compared visually using t-SNE and UMAP plots.

### üèÜ Results & Insights
| Approach                      | Key Highlights                                                  | Best Performing Model    |
| ----------------------------- | --------------------------------------------------------------- | ------------------------ |
| **Manual ML (Balanced Data)** | Improved minority class detection after SMOTE                   | XGBoost / LightGBM       |
| **PyCaret (Supervised)**      | Auto-tuning gave similar or higher recall with less manual work | Random Forest / CatBoost |
| **Anomaly Detection (Unsupervised)**    | Isolation Forest effectively isolated fraud cases               | Isolation Forest         |

- ‚úÖ Balancing techniques (especially SMOTE) significantly improved recall.
- ‚úÖ PyCaret reduced modeling time drastically with automated comparisons.
- ‚úÖ Unsupervised models offered valuable insight into unknown fraud patterns.


